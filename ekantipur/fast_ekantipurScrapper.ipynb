{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        return f\"HTTP error occurred: {http_err}\"\n",
    "    except Exception as err:\n",
    "        return f\"An error occurred: {err}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link(url, date, error_urls):\n",
    "    try:\n",
    "        html_content = fetch_url(url + date)\n",
    "        if not html_content.startswith(\"HTTP error\") and not html_content.startswith(\"An error\"):\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            mydivs = soup.find_all(\"div\", class_=\"teaser offset\")\n",
    "            with open(\"oneYearNews8.txt\", \"a+\") as f:\n",
    "                for div in mydivs:\n",
    "                    f.write(\"https://ekantipur.com\" + div.find('a')['href'] + \"\\n\")\n",
    "        else:\n",
    "            error_urls.append(url + date)\n",
    "    except Exception as err:\n",
    "        error_urls.append(url + date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    url = \"https://ekantipur.com/pradesh-7/\"\n",
    "    start_date = date(2023, 1, 1)\n",
    "    end_date = date(2023, 12, 31)\n",
    "    delta = timedelta(days=1)\n",
    "    cus_date = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n",
    "    error_urls = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        executor.map(lambda x: get_link(url, x.strftime(\"%Y/%m/%d\"), error_urls), cus_date)\n",
    "\n",
    "    with open(\"error_links.txt\", \"a+\") as f:\n",
    "        for error_url in error_urls:\n",
    "            f.write(error_url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
